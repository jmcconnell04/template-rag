### Health Check
# Checks if the server is running and healthy.
GET http://localhost:8000/health
Accept: application/json

### Root Endpoint
# Displays a welcome message and some configuration details.
GET http://localhost:8000/
Accept: application/json

### Test Ollama Prompt (gemma:2b - default model for the endpoint)
# Sends a prompt to Ollama using the default model specified in the server's endpoint logic (gemma:2b).
# Ensure you have pulled 'gemma:2b' into your Ollama instance first:
# docker exec -it YOUR_WORKSPACE_NAME-ollama-1 ollama pull gemma:2b
POST http://localhost:8000/test_ollama_prompt
Content-Type: application/json

{
  "prompt": "Why is the sky blue during the day?"
}

### Test Ollama Prompt (explicitly gemma:2b)
# Same as above, but explicitly specifies gemma:2b
POST http://localhost:8000/test_ollama_prompt
Content-Type: application/json

{
  "model": "gemma:2b",
  "prompt": "What are the main components of a cell?"
}

### Test Ollama Prompt (qwen:1.8b - if pulled)
# Sends a prompt to Ollama using the qwen:1.8b model.
# Ensure you have pulled 'qwen:1.8b' into your Ollama instance first:
# docker exec -it YOUR_WORKSPACE_NAME-ollama-1 ollama pull qwen:1.8b
POST http://localhost:8000/test_ollama_prompt
Content-Type: application/json

{
  "model": "qwen:1.8b",
  "prompt": "Tell me a short story about a robot who learns to paint."
}

### Test Ollama Prompt (llama3:70b - if pulled and you have resources)
# Sends a prompt to Ollama using the llama3:70b model.
# This is a large model and requires significant resources.
# Ensure you have pulled 'llama3:70b' into your Ollama instance first:
# docker exec -it YOUR_WORKSPACE_NAME-ollama-1 ollama pull llama3:70b
POST http://localhost:8000/test_ollama_prompt
Content-Type: application/json

{
  "model": "llama3:70b",
  "prompt": "Explain the theory of general relativity in simple terms."
}


### Test OpenAI Non-Streaming
POST http://localhost:8000/v1/chat/completions 
Content-Type: application/json
# Authorization: Bearer YOUR_IF_NEEDED_TOKEN_FOR_YOUR_SERVER

{
  "model": "openai/gpt-3.5-turbo",
  "messages": [
    {"role": "user", "content": "What is the capital of Canada in a single word?"}
  ],
  "stream": false
}

### Test Gemini Non-Streaming
POST http://localhost:8000/v1/chat/completions
Content-Type: application/json

{
  "model": "gemini/gemini-1.5-flash-latest",
  "messages": [
    {"role": "user", "content": "What are three primary colors?"}
  ],
  "stream": false
}



# Goal: Allow users to upload documents via OpenWebUI, which are then processed and added to the RAG knowledge base of their currently active project.
# Action:
# Investigate OpenWebUI's document upload mechanism (expected endpoint, payload format).
# Create a new API endpoint in server/app/main.py (e.g., /v1/documents/upload).
# This endpoint will use the active project context and call rag_service.add_document_to_project_collection().
# Basic Audit Logging Implementation (Expand):

# Goal: Populate the AuditLogs table in SQLite to demonstrate auditability.
# Action: Ensure audit_service.py and the AuditLog model are in place. Integrate more calls to audit_service.log_action() at key points (project switch, document upload, RAG query, LLM call, user identification).
# README.md Finalization:

# Goal: A comprehensive user guide.
# Action: Continuously update with setup instructions (SQLite, PostgreSQL), configuration details, GPU usage, feature explanations (like the !use_project command), and a basic Azure App Service PoC deployment guide.
# Azure OpenAI & Azure AI Search Integration (Post v1 Polish):

# Goal: Provide an alternative RAG backend strategy using Azure's PaaS services.
# Status: You've decided to insert this after the current core items are polished. This remains a future enhancement.
